{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mariam Gasoyan\n",
    "# Homework 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import requests\n",
    "#import numpy as np\n",
    "import pandas as pd\n",
    "from textblob import TextBlob, Word\n",
    "from scrapy.http import TextResponse\n",
    "import nltk\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "#from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "#from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "#from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#from selenium import webdriver\n",
    "#import selenium.webdriver.common.keys as Keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  1.  Scrape all the latest news headings (and publication time) from https://www.tert.am/en and answer the following questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.tert.am/en\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(url)\n",
    "response.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = TextResponse(url = response.url, body = response.text, encoding = \"utf-8\")\n",
    "latest_news = response.xpath('//ul[@class=\"list list--with-scroll scroller-block show-link-visited\"]//span[@class=\"list__title db\"]/text()').extract()\n",
    "print(len(latest_news))\n",
    "latest_news[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) What are the top most frequent words? (not stopword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw = stopwords.words(\"english\")\n",
    "word_list = []\n",
    "for sentense in latest_news:\n",
    "    sentense = sentense.replace(\"'\", \"\").replace(\"‘\", \"\").replace(\"’\", \"\").replace(\"–\", \" \").replace(\".\", \"\")\n",
    "    sentense=TextBlob(sentense)\n",
    "    sentense.words = sentense.words.lemmatize()\n",
    "    for word in sentense.words:\n",
    "        word = word.lemmatize(\"v\").lower()        \n",
    "        if word not in sw:\n",
    "            word_list.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(word_list)\n",
    "print(\"Top most frequent words: \\n------------------------\")\n",
    "df[0].value_counts()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " b) How many typical Armenian surnames can be counted in titles? (assume surname in\n",
    "any word starting with an uppercase and ending with “yan”),\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surnames = []\n",
    "for sentense in latest_news:\n",
    "    surnames.extend(re.findall('[A-Z][a-z]+yan', sentense))\n",
    "print(len(surnames), 'surnames:', surnames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) In what part of the day are most of the articles published? (you may provide some\n",
    "range such as from 10am to 11am).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times_draft = response.xpath('//ul[@class=\"list list--with-scroll scroller-block show-link-visited\"]//span[@class=\"list__date db fb fs14\"]/text()').extract()\n",
    "times_draft[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times = []\n",
    "for t in times_draft:\n",
    "    times.append(int(re.findall('\\d\\d:', t)[0].replace(':', '')))\n",
    "    times_df = pd.DataFrame(times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times_dist = nltk.FreqDist(times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_hours = nltk.Counter(times_dist).most_common()\n",
    "common_hours[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Most common hours:\", str(common_hours[0][0])+\":00-\" + str(common_hours[0][0]+1) +\":00,\", common_hours[0][1], \"times.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Scrape all jobs and companies from https://staff.am/en/jobs and answer the following questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_urls = [\"https://staff.am/en/jobs?page={}&per-page=50\".format(i) for i in range(1,14)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "requests.get(all_urls[0]).status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def job_scraper(url):\n",
    "    response = requests.get(url)\n",
    "    response = TextResponse(url = response.url, body = response.text, encoding = \"utf-8\")\n",
    "    jobs = response.xpath('//p[@class=\"font_bold\"]/text()').extract()\n",
    "    return jobs\n",
    "\n",
    "all_jobs = []\n",
    "for url in all_urls:\n",
    "    all_jobs.extend(job_scraper(url))\n",
    "    \n",
    "len(all_jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def company_scraper(url):\n",
    "    response = requests.get(url)\n",
    "    response = TextResponse(url = response.url, body = response.text, encoding = \"utf-8\")\n",
    "    companies = response.xpath('//p[@class=\"job_list_company_title\"]/text()').extract()\n",
    "    return companies\n",
    "\n",
    "all_companies = []\n",
    "for url in all_urls:\n",
    "    all_companies.extend(company_scraper(url))\n",
    "    \n",
    "len(all_companies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_jobs) == len(all_companies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Which company has the highest number of job postings?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "companies_df = pd.DataFrame(all_companies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_company = list(dict(companies_df[0].value_counts()[:1]).keys())[0]\n",
    "print(\"The top company:\", top_company)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) How many postings does this company have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(top_company, 'has', companies_df[0].value_counts()[0], 'jobs posted.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Which company has the highest number of “developer” (or related) job postings?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_list_indices = []\n",
    "for i in range(len(all_jobs)):\n",
    "    dev = re.findall(\".eveloper[a-z]*\", all_jobs[i])\n",
    "    if len(dev) >0:\n",
    "        dev_list_indices.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_companies = []\n",
    "for i in dev_list_indices:\n",
    "    dev_companies.append(all_companies[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_companies_df = pd.DataFrame(dev_companies)\n",
    "top_dev_company = list(dict(dev_companies_df[0].value_counts()[:1]).keys())[0]\n",
    "print(top_dev_company, \"has the highest number of “developer” (or related) job postings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Scrape the table with exchange rates from www.rates.am for the first 2 weeks of July \n",
    "#### (can be done both with and without Selenium)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get('http://rates.am/en')\n",
    "response.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "days = []\n",
    "for i in range(2, 16):\n",
    "    if i < 10:\n",
    "        days.append(\"0\" + str(i))\n",
    "    else:\n",
    "        days.append(str(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as there was no specification in the assignment, I'm going to scrape the cash exchange rates of all exchange points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ex_urls = []\n",
    "for d in days:\n",
    "    all_ex_urls.append('http://rates.am/en/armenian-dram-exchange-rates/exchange-points/cash/2019/07/'+d+'/20-00')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "averages = []\n",
    "for url in all_ex_urls:\n",
    "    table = pd.read_html(url)[3]\n",
    "    buy_list =list(pd.to_numeric(table[5].dropna()[2:-1]))\n",
    "    sell_list =list(pd.to_numeric(table[5].dropna()[2:-1]))\n",
    "    avg = (sum(buy_list) + sum(buy_list))/len(buy_list)/2\n",
    "    averages.append(avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) What is the average USD rate for this duration?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_rate = sum(averages)/len(averages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Average rate:\", average_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Plot the average daily USD rate over time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_rates_df = pd.DataFrame(averages, days) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_rates_df.plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
